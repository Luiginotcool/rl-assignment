We can model the robot's decision making process with a Markov Decision Process, with:
a set of states S representing the possible positions the robot can have, 
a set of actions A available to the robot, 
A transition probability function P(s’ | s,a) that defines the likelihood of moving to state s’ after taking action a in state s,
A reward function R(s,a,s’) specifying the immediate reward received when taking action a in state s and transitioning to state s’

The robot starts from a state s with the goal of selecting actions that maximise the expected return. The agent selects actions according to a policy PI(a|s).
The total return the agent receives is defined as: 
G_t = R_t+1 + gamma R_t+2 + gamma^2 R_t+3 + …
And G_t+1 = R_t+2 + gamma R_t+3 + …
=> G_t = R_t+1 + gamma G_t+1

We can denote the expected return of taking an action a in state s as q^pi(a,s) defined as 
q^pi(s,a) = E_pi(G_t | S_t = s, A_t = a)
This is equivalent to saying
q^pi(s,a) + E_pi(R_t+1 + gamma q^pi(s’, a’) | S_t = a, A_t = a)
=> E_pi(R_t+1 + gamma q^pi(s’, a’) - q^pi(s, a) | S_t = a, A_t = a) = 0

In order to maximise the expected return in any given state, the robot should select the action which maximises q^pi(s, a). However, the true values of q^pi(a,s) are not known to the robot. We therefore introduce a parameterised Q(s,a) to estimate them. The expectation then becomes approximately zero:
E_pi(R_t+1 + gamma Q^pi(s’, a’) - Q^pi(s, a) | S_t = a, A_t = a) = 0

We define a squared loss function around this quantity in order to make it minimizable. Minimising this loss function will provide us with an approximation of q^pi which the robot can use to take actions which will maximise the estimated return.

For a single transition (s, a, r, s’, a’) the loss function is:
L(s,a)  = ½ (Q(s,a) - (r + gamma Q(s’, a’)))^2

Assuming Q(s’, a’) is known and fixed, we take the partial derivative of the loss function with respect to Q(s,a):
DERIVATIVE OF LOSS FUNCTION*

To update Q(s,a) we apply:
	**ONLINE UPDATE RULE**

Using this update rule we can write an algorithm to estimate q^pi, and maximise the reward the robot receives:





Initialise Q(s,a) arbitrarily for all s in S and a in A.
For each episode:
	Initialise state s
	Choose action a from s using a policy derived from Q
	For each step of the episode:
		Take action a, observe reward r and next state s’
		If s’ is terminal:
			Update Q(s,a): Q(s,a) <- Q(s,a) + eta*(r-Q(s,a))
			Break
		Choose action a’ from s’ using a policy derived from Q
Update Q(s,a): Q(s,a) <- Q(s,a) + eta * (r + gamma Q(s’,a’) - Q(s,a)
Update state and action: s <- s’, a <- a’



SERSA algorithm with epsilon-greedy policy:
One goal
Two goals, doesn’t work

Increase steps, change gamma

Increase epsilon so that it reaches far goal

Add epsilon schedule 1, linear decay over steps
Reaches far goal, doesn’t reach it consistently

Try SERSA lambda algorithm
Takes much longer
Didn’t converge in feasible timespan

Add epsilon schedule 2, staged over episodes
Reaches far goal consistently


PLOT 1:
Rewards:
-100.0 458
0.0 28
5000.0 4514
Params:
  n_episodes = 5000,
  eta= 0.5,
  gamma = 1 - 0.01,
  epsilon = 0.1
Environment:
goal_locations=[(1, 2)], goal_rewards=[5000]



PLOT 2:
Rewards:
-100.0 5
0.0 2
250.0 4993
Params:
	n_episodes = 5000,
  	eta= 0.5,
  	gamma = 1 - 0.01,
  	epsilon = 0.1
Environment:
goal_locations=[(1,2), (8,11)], goal_rewards=[5000, 250]

PLOT 3:
Rewards:
-100.0 24
250.0 4976
Params:
	n_episodes = 5000,
  	eta= 0.5,
  	gamma = 1 - 0.01,
  	epsilon = 0.5
Environment:
goal_locations=[(1,2), (8,11)], goal_rewards=[5000, 250]

PLOT 4:
Rewards:
-100.0 983
0.0 1
250.0 4009
5000.0 7
Params:
	n_episodes = 5000,
  	eta= 0.5,
  	gamma = 1 - 0.01,
  	epsilon = 0.9
Environment:
goal_locations=[(1,2), (8,11)], goal_rewards=[5000, 250]









PLOT 5:
Rewards:
-100.0 179
0.0 179
250.0 4638
5000.0 4
Params:
	n_episodes = 5000,
  	eta= 0.98,
  	gamma = 1 - 0.01,
  	epsilon_start = 1,
  	epsilon_decay = 65,
Environment:
goal_locations=[(1,2), (8,11)], goal_rewards=[5000, 250]

PLOT 6:
Rewards:
-100.0 280
0.0 1455
250.0 3257
5000.0 8
Params:
  	n_episodes = 5000,
  	eta = 0.99,
  	gamma = 1,
  	epsilon_start = 1,
  	epsilon_decay = 85
Environment:
goal_locations=[(1,2), (8,11)], goal_rewards=[5000, 250]

PLOT 7:
Rewards:
-100.0 4
250.0 996
Params:
	n_episodes = 1000,
  	eta= 0.98,
  	gamma = 1 - 0.01,
  	epsilon_start = 1,
  	epsilon_decay = 250,
  	lambda_ = 0.5
Environment:
goal_locations=[(1,2), (8,11)], goal_rewards=[5000, 250]






PLOT 8:
Rewards:
-100.0 25
0.0 12
250.0 4945
5000.0 18

Params:
    	n_episodes = 5000,
    	eta= 0.95,
    	gamma = 1 - 0.01,
   	stage_2_time=200,
   	stage_2_rate=0.1
Environment:
goal_locations=[(1,2), (8,11)], goal_rewards=[5000, 250]

PLOT 9:
Rewards:
-100.0 27
0.0 101
250.0 821
5000.0 4051

Params:
    	n_episodes = 5000,
    	eta= 0.95,
    	gamma = 1 - 0.01,
   	stage_2_time=1000,
   	stage_2_rate=0.1
Environment:
goal_locations=[(1,2), (8,11)], goal_rewards=[5000, 250]

PLOT 10:
Rewards:
-100.0 27
0.0 41
250.0 800
5000.0 4132
Params:
    	n_episodes = 5000,
    	eta= 0.5,
    	gamma = 1 - 0.01,
   	stage_2_time=1000,
   	stage_2_rate=0
Environment:
goal_locations=[(1,2), (8,11)], goal_rewards=[5000, 250]

























PLOT X:
Rewards:

Params:

Environment:


PLOT X:
Rewards:

Params:

Environment:


PLOT X:
Rewards:

Params:

Environment: